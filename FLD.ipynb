{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from detection import train_detector, detect, read_csv\n",
    "\n",
    "train_dir = '00_input/train'\n",
    "im_size = 140\n",
    "coords_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from keras.layers import (Input, concatenate, Conv2D, MaxPooling2D, \n",
    "                          UpSampling2D, Convolution2D, ZeroPadding2D, \n",
    "                          BatchNormalization, Activation, concatenate, \n",
    "                          Flatten, Dense, merge, Dropout)\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input(shape=(im_size, im_size, 1))\n",
    "    dropout = Dropout(0.2)(inputs)\n",
    "    conv = Conv2D(filters=16, kernel_size=(3,3), padding='same')(dropout)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=32, kernel_size=(3,3), padding='same')(batchnorm)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=48, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    dropout = Dropout(0.3)(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(dropout)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=96, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=128, kernel_size=(3,3), padding='same')(batchnorm)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    dropout = Dropout(0.3)(maxpool)\n",
    "    \n",
    "    conv = Conv2D(filters=192, kernel_size=(3,3), padding='same')(dropout)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=256, kernel_size=(3,3), padding='same')(batchnorm)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=384, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=512, kernel_size=(3,3), padding='same')(batchnorm)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    dropout = Dropout(0.5)(maxpool)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    predictions = Dense(coords_size, activation=None)(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(0.001, decay=0.00002), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 160, 160, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 160, 160, 16)      160       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 160, 160, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 160, 160, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 160, 160, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 160, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 160, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 80, 80, 48)        13872     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 80, 80, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 80, 80, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 80, 80, 64)        27712     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 40, 40, 96)        55392     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 40, 40, 128)       110720    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 20, 20, 192)       221376    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 256)       442624    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 10, 10, 384)       885120    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                1075228   \n",
      "=================================================================\n",
      "Total params: 2,841,708\n",
      "Trainable params: 2,839,276\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 35s 7ms/step - loss: 623.0597 - val_loss: 107.1390\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 107.13897, saving model to exp_1_107.1390.hdf5\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 56.0199 - val_loss: 66.3952\n",
      "\n",
      "Epoch 00002: val_loss improved from 107.13897 to 66.39523, saving model to exp_2_66.3952.hdf5\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 47.1184 - val_loss: 178.1905\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 66.39523\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 43.6915 - val_loss: 114.1505\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 66.39523\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 39.2671 - val_loss: 94.0265\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 66.39523\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 36.9054 - val_loss: 111.1778\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 66.39523\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 35.5313 - val_loss: 78.8376\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 66.39523\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 31.9665 - val_loss: 88.8998\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 66.39523\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 28.8703 - val_loss: 105.3992\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 66.39523\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 26.9890 - val_loss: 97.7225\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 66.39523\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 25.3674 - val_loss: 111.2953\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 66.39523\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 23.7687 - val_loss: 350.1808\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 66.39523\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 21.5814 - val_loss: 210.2009\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 66.39523\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 20.8776 - val_loss: 37.0655\n",
      "\n",
      "Epoch 00014: val_loss improved from 66.39523 to 37.06547, saving model to exp_14_37.0655.hdf5\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 18.3566 - val_loss: 205.8820\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 37.06547\n",
      "Epoch 16/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 17.3108 - val_loss: 59.6641\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 37.06547\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 16.1316 - val_loss: 76.0972\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 37.06547\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 15.7714 - val_loss: 38.9167\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 37.06547\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 15.2279 - val_loss: 629.5359\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 37.06547\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 13.8690 - val_loss: 292.1126\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 37.06547\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 13.8754 - val_loss: 113.1133\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 37.06547\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 13.2039 - val_loss: 34.5838\n",
      "\n",
      "Epoch 00022: val_loss improved from 37.06547 to 34.58384, saving model to exp_22_34.5838.hdf5\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 13.1194 - val_loss: 377.8715\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 34.58384\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 12.5493 - val_loss: 61.0613\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 34.58384\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 13.2732 - val_loss: 1635.4562\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 34.58384\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 11.9620 - val_loss: 79.3418\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 34.58384\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 11.5631 - val_loss: 27.7122\n",
      "\n",
      "Epoch 00027: val_loss improved from 34.58384 to 27.71223, saving model to exp_27_27.7122.hdf5\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 11.0546 - val_loss: 3737.8969\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 27.71223\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 11.4884 - val_loss: 172.2882\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 27.71223\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 10.5631 - val_loss: 147.4458\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 27.71223\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 10.2414 - val_loss: 482.4131\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 27.71223\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 10.0455 - val_loss: 174.1778\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.71223\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 10.1455 - val_loss: 56.6736\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 27.71223\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 10.2852 - val_loss: 97.9513\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 27.71223\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 10.2875 - val_loss: 23.6159\n",
      "\n",
      "Epoch 00035: val_loss improved from 27.71223 to 23.61590, saving model to exp_35_23.6159.hdf5\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 9.6415 - val_loss: 32.6985\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.61590\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 10.0180 - val_loss: 26.2818\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.61590\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 10.1215 - val_loss: 583.5130\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.61590\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 9.4850 - val_loss: 23.6728\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.61590\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 9.5735 - val_loss: 57.3035\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.61590\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 9.2347 - val_loss: 1281.7535\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.61590\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.7456 - val_loss: 1762.5054\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.61590\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.7050 - val_loss: 39.4441\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.61590\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.7905 - val_loss: 794.4150\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.61590\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.4282 - val_loss: 70.0719\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.61590\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.5187 - val_loss: 31.9089\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.61590\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.5841 - val_loss: 1264.2057\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.61590\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.5484 - val_loss: 239.7238\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.61590\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.3343 - val_loss: 4321.6479\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.61590\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 8.0358 - val_loss: 707.9009\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.61590\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 7.9388 - val_loss: 964.1757\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23.61590\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 7.5792 - val_loss: 157.0837\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23.61590\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 7.3539 - val_loss: 32.3138\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23.61590\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 7.7551 - val_loss: 177.8065\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23.61590\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 7.9837 - val_loss: 795.3687\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23.61590\n",
      "Epoch 56/100\n",
      "1100/5000 [=====>........................] - ETA: 20s - loss: 7.2161\n",
      "Training interrupted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fda583b75c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gt = read_csv(train_dir+'/gt.csv')\n",
    "train_detector(train_gt, train_dir+'/images', False, get_model, 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 160, 160, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 160, 160, 16)      160       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 160, 160, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 160, 160, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 160, 160, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 160, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 160, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 80, 80, 48)        13872     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 80, 80, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 80, 80, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 80, 80, 64)        27712     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 40, 40, 96)        55392     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 40, 40, 128)       110720    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 20, 20, 192)       221376    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 256)       442624    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 10, 10, 384)       885120    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                1075228   \n",
      "=================================================================\n",
      "Total params: 2,841,708\n",
      "Trainable params: 2,839,276\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
