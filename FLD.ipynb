{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from detection import train_detector, detect, read_csv\n",
    "\n",
    "train_dir = '00_input/train'\n",
    "im_size = 128\n",
    "coords_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 11, 13, 15])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "                [9, 10, 11, 12, 13, 14, 15, 16]])\n",
    "arr[1][::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from keras.layers import (Input, concatenate, Conv2D, MaxPooling2D, \n",
    "                          UpSampling2D, Convolution2D, ZeroPadding2D, \n",
    "                          BatchNormalization, Activation, concatenate, \n",
    "                          Flatten, Dense, merge, Dropout)\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input(shape=(im_size, im_size, 1))\n",
    "    dropout = Dropout(0.2)(inputs)\n",
    "    conv = Conv2D(filters=16, kernel_size=(3,3), padding='same')(dropout)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=32, kernel_size=(3,3), padding='same')(batchnorm)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=48, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    dropout = Dropout(0.3)(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(dropout)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=96, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=128, kernel_size=(3,3), padding='same')(batchnorm)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    dropout = Dropout(0.3)(maxpool)\n",
    "    \n",
    "    conv = Conv2D(filters=192, kernel_size=(3,3), padding='same')(dropout)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=256, kernel_size=(3,3), padding='same')(batchnorm)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    maxpool = MaxPooling2D()(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=384, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    relu = Activation('relu')(conv)\n",
    "    batchnorm = BatchNormalization()(relu)\n",
    "    \n",
    "    dropout = Dropout(0.5)(batchnorm)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    predictions = Dense(coords_size, activation=None)(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(0.001, decay=0.00002), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 16)      160       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 48)        13872     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 48)        192       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 64, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 64)        27712     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 96)        55392     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 128)       110720    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 192)       221376    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 256)       442624    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 8, 8, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 384)         1536      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 384)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 24576)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                688156    \n",
      "=================================================================\n",
      "Total params: 2,454,636\n",
      "Trainable params: 2,452,204\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 24s 5ms/step - loss: 420.5970 - val_loss: 134.3385\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 134.33845, saving model to exp_1_134.3385.hdf5\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 49.7336 - val_loss: 93.0043\n",
      "\n",
      "Epoch 00002: val_loss improved from 134.33845 to 93.00429, saving model to exp_2_93.0043.hdf5\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 41.4278 - val_loss: 114.8340\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 93.00429\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 37.7463 - val_loss: 62.9952\n",
      "\n",
      "Epoch 00004: val_loss improved from 93.00429 to 62.99518, saving model to exp_4_62.9952.hdf5\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 34.4938 - val_loss: 57.2189\n",
      "\n",
      "Epoch 00005: val_loss improved from 62.99518 to 57.21891, saving model to exp_5_57.2189.hdf5\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 31.5596 - val_loss: 55.7241\n",
      "\n",
      "Epoch 00006: val_loss improved from 57.21891 to 55.72414, saving model to exp_6_55.7241.hdf5\n",
      "Epoch 7/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 29.9640 - val_loss: 65.3144\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 55.72414\n",
      "Epoch 8/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 29.1875 - val_loss: 85.6058\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 55.72414\n",
      "Epoch 9/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 27.4427 - val_loss: 56.7681\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 55.72414\n",
      "Epoch 10/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 26.0289 - val_loss: 45.1883\n",
      "\n",
      "Epoch 00010: val_loss improved from 55.72414 to 45.18831, saving model to exp_10_45.1883.hdf5\n",
      "Epoch 11/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 24.7230 - val_loss: 30.8174\n",
      "\n",
      "Epoch 00011: val_loss improved from 45.18831 to 30.81741, saving model to exp_11_30.8174.hdf5\n",
      "Epoch 12/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 24.0136 - val_loss: 43.0408\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 30.81741\n",
      "Epoch 13/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 22.5008 - val_loss: 32.6115\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 30.81741\n",
      "Epoch 14/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 21.5585 - val_loss: 28.6623\n",
      "\n",
      "Epoch 00014: val_loss improved from 30.81741 to 28.66232, saving model to exp_14_28.6623.hdf5\n",
      "Epoch 15/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 21.2016 - val_loss: 28.6711\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 28.66232\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 19s 4ms/step - loss: 20.3855 - val_loss: 44.5141\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 28.66232\n",
      "Epoch 17/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 19.5247 - val_loss: 31.2226\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 28.66232\n",
      "Epoch 18/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 17.4030 - val_loss: 40.5848\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 28.66232\n",
      "Epoch 19/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 17.4025 - val_loss: 23.1255\n",
      "\n",
      "Epoch 00019: val_loss improved from 28.66232 to 23.12548, saving model to exp_19_23.1255.hdf5\n",
      "Epoch 20/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 16.3017 - val_loss: 21.8859\n",
      "\n",
      "Epoch 00020: val_loss improved from 23.12548 to 21.88594, saving model to exp_20_21.8859.hdf5\n",
      "Epoch 21/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 15.3249 - val_loss: 23.3036\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 21.88594\n",
      "Epoch 22/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 14.2807 - val_loss: 28.8819\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 21.88594\n",
      "Epoch 23/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 13.4388 - val_loss: 20.4252\n",
      "\n",
      "Epoch 00023: val_loss improved from 21.88594 to 20.42516, saving model to exp_23_20.4252.hdf5\n",
      "Epoch 24/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 13.3098 - val_loss: 20.4122\n",
      "\n",
      "Epoch 00024: val_loss improved from 20.42516 to 20.41221, saving model to exp_24_20.4122.hdf5\n",
      "Epoch 25/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 11.8987 - val_loss: 21.9838\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 20.41221\n",
      "Epoch 26/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 11.8105 - val_loss: 19.6827\n",
      "\n",
      "Epoch 00026: val_loss improved from 20.41221 to 19.68274, saving model to exp_26_19.6827.hdf5\n",
      "Epoch 27/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 11.1989 - val_loss: 20.5976\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 19.68274\n",
      "Epoch 28/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 11.0483 - val_loss: 25.6757\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 19.68274\n",
      "Epoch 29/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 11.4382 - val_loss: 21.5152\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 19.68274\n",
      "Epoch 30/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 10.6368 - val_loss: 20.8264\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 19.68274\n",
      "Epoch 31/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 10.2816 - val_loss: 20.5989\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 19.68274\n",
      "Epoch 32/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 10.1300 - val_loss: 18.9320\n",
      "\n",
      "Epoch 00032: val_loss improved from 19.68274 to 18.93203, saving model to exp_32_18.9320.hdf5\n",
      "Epoch 33/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 10.3770 - val_loss: 20.0634\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 18.93203\n",
      "Epoch 34/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 9.6286 - val_loss: 17.8309\n",
      "\n",
      "Epoch 00034: val_loss improved from 18.93203 to 17.83091, saving model to exp_34_17.8309.hdf5\n",
      "Epoch 35/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 9.4767 - val_loss: 18.0766\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 17.83091\n",
      "Epoch 36/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 9.3321 - val_loss: 20.0960\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 17.83091\n",
      "Epoch 37/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 9.4674 - val_loss: 19.2931\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 17.83091\n",
      "Epoch 38/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 9.0923 - val_loss: 19.1663\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 17.83091\n",
      "Epoch 39/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 8.5621 - val_loss: 17.0129\n",
      "\n",
      "Epoch 00039: val_loss improved from 17.83091 to 17.01289, saving model to exp_39_17.0129.hdf5\n",
      "Epoch 40/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 8.5274 - val_loss: 19.3358\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 17.01289\n",
      "Epoch 41/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 8.5785 - val_loss: 18.6376\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 17.01289\n",
      "Epoch 42/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 8.0512 - val_loss: 18.2407\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 17.01289\n",
      "Epoch 43/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.8312 - val_loss: 18.4698\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 17.01289\n",
      "Epoch 44/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.8910 - val_loss: 19.9529\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 17.01289\n",
      "Epoch 45/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 8.6353 - val_loss: 17.7150\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 17.01289\n",
      "Epoch 46/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.7770 - val_loss: 17.7962\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 17.01289\n",
      "Epoch 47/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 8.0115 - val_loss: 17.7981\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 17.01289\n",
      "Epoch 48/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.9807 - val_loss: 16.6949\n",
      "\n",
      "Epoch 00048: val_loss improved from 17.01289 to 16.69485, saving model to exp_48_16.6949.hdf5\n",
      "Epoch 49/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.3438 - val_loss: 17.1189\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 16.69485\n",
      "Epoch 50/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.2340 - val_loss: 16.9152\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 16.69485\n",
      "Epoch 51/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.4404 - val_loss: 18.7368\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 16.69485\n",
      "Epoch 52/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.7671 - val_loss: 17.4882\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 16.69485\n",
      "Epoch 53/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.1134 - val_loss: 18.2555\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 16.69485\n",
      "Epoch 54/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.9332 - val_loss: 17.0043\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 16.69485\n",
      "Epoch 55/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.0562 - val_loss: 18.0515\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 16.69485\n",
      "Epoch 56/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.0056 - val_loss: 17.9565\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 16.69485\n",
      "Epoch 57/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.2024 - val_loss: 18.6635\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 16.69485\n",
      "Epoch 58/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.0206 - val_loss: 17.7663\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 16.69485\n",
      "Epoch 59/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 7.1687 - val_loss: 18.6817\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 16.69485\n",
      "Epoch 60/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.9550 - val_loss: 16.9038\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 16.69485\n",
      "Epoch 61/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.7535 - val_loss: 17.2633\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 16.69485\n",
      "Epoch 62/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.6591 - val_loss: 17.0363\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 16.69485\n",
      "Epoch 63/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.8089 - val_loss: 16.6443\n",
      "\n",
      "Epoch 00063: val_loss improved from 16.69485 to 16.64429, saving model to exp_63_16.6443.hdf5\n",
      "Epoch 64/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.6030 - val_loss: 17.0052\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 16.64429\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.5175 - val_loss: 17.7729\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 16.64429\n",
      "Epoch 66/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.4470 - val_loss: 17.1497\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 16.64429\n",
      "Epoch 67/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.6245 - val_loss: 17.0986\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 16.64429\n",
      "Epoch 68/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.6035 - val_loss: 19.5452\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 16.64429\n",
      "Epoch 69/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.3119 - val_loss: 17.8307\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 16.64429\n",
      "Epoch 70/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.1333 - val_loss: 16.4339\n",
      "\n",
      "Epoch 00070: val_loss improved from 16.64429 to 16.43388, saving model to exp_70_16.4339.hdf5\n",
      "Epoch 71/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.1347 - val_loss: 17.5631\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 16.43388\n",
      "Epoch 72/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.0411 - val_loss: 16.1948\n",
      "\n",
      "Epoch 00072: val_loss improved from 16.43388 to 16.19477, saving model to exp_72_16.1948.hdf5\n",
      "Epoch 73/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 5.9259 - val_loss: 16.6004\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 16.19477\n",
      "Epoch 74/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.1555 - val_loss: 16.7627\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 16.19477\n",
      "Epoch 75/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.0989 - val_loss: 18.0597\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 16.19477\n",
      "Epoch 76/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.2193 - val_loss: 17.1890\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 16.19477\n",
      "Epoch 77/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.0964 - val_loss: 17.5031\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 16.19477\n",
      "Epoch 78/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.0956 - val_loss: 16.8107\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 16.19477\n",
      "Epoch 79/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.2662 - val_loss: 16.5057\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 16.19477\n",
      "Epoch 80/100\n",
      "5000/5000 [==============================] - 19s 4ms/step - loss: 6.0641 - val_loss: 16.8995\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 16.19477\n",
      "Epoch 81/100\n",
      "1500/5000 [========>.....................] - ETA: 12s - loss: 6.7165\n",
      "Training interrupted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f2bcc4fdc50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gt = read_csv(train_dir+'/gt.csv')\n",
    "train_detector(train_gt, train_dir+'/images', False, get_model, 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 16)      160       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 48)        13872     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 48)        192       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 64, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 64)        27712     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 96)        55392     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 128)       110720    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 192)       221376    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 256)       442624    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 8, 8, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 384)         1536      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 384)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 24576)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                688156    \n",
      "=================================================================\n",
      "Total params: 2,454,636\n",
      "Trainable params: 2,452,204\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
