{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dir = '00_input/train'\n",
    "im_size = 100\n",
    "coords_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    from numpy import array\n",
    "    res = {}\n",
    "    with open(filename) as fhandle:\n",
    "        next(fhandle)\n",
    "        for line in fhandle:\n",
    "            parts = line.rstrip('\\n').split(',')\n",
    "            coords = array([float(x) for x in parts[1:]], dtype='float64')\n",
    "            res[parts[0]] = coords\n",
    "    return res\n",
    "\n",
    "train_gt = read_csv(train_dir+'/gt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(train_gt, train_img_dir, info=False):\n",
    "    from skimage.data import imread\n",
    "    from scipy.ndimage import zoom\n",
    "    train_X = np.zeros((len(train_gt), im_size, im_size, 1))\n",
    "    train_Y = np.zeros((len(train_gt), coords_size))\n",
    "    for i, img_name in enumerate(train_gt):\n",
    "        img = imread(train_img_dir+'/'+img_name, as_grey=True)\n",
    "        train_Y[i] = train_gt[img_name]\n",
    "        for j in range(1, coords_size, 2):\n",
    "            train_Y[i][j] *= im_size/img.shape[0]\n",
    "        for j in range(0, coords_size, 2):\n",
    "            train_Y[i][j] *= im_size/img.shape[1]\n",
    "        train_Y[i] = (train_Y[i] / 100) \n",
    "        img = zoom(img, [im_size/img.shape[0], im_size/img.shape[1]])\n",
    "        img = (img / 255) \n",
    "        train_X[i,:,:,0] = img\n",
    "        del(img)\n",
    "        if info and (i+1)%100 == 0:\n",
    "            print('Image: ', i+1, end='\\r')\n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from keras.layers import (Input, concatenate, Conv2D, MaxPooling2D, \n",
    "                          UpSampling2D, Convolution2D, ZeroPadding2D, \n",
    "                          BatchNormalization, Activation, concatenate, \n",
    "                          Flatten, Dense, merge)\n",
    "from keras.optimizers import rmsprop\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input(shape=(im_size, im_size, 1))\n",
    "    conv = Conv2D(filters=256, kernel_size=(3,3), padding='same')(inputs)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=128, kernel_size=(3,3), padding='same')(relu)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    maxpool = MaxPooling2D()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(relu)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    maxpool = MaxPooling2D()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=32, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    flatten = Flatten()(relu)\n",
    "    predictions = Dense(coords_size, activation='tanh')(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_detector(\n",
    "        train_gt, \n",
    "        train_img_dir, \n",
    "        fast_train=False, \n",
    "        model_func=None, \n",
    "        model_name='{epoch:d}_{val_loss:.2f}.hdf5'):\n",
    "    train_X, train_Y = parse(train_gt, train_img_dir, True)\n",
    "    if model_func == None:\n",
    "        model = get_model()\n",
    "    else:\n",
    "        model = model_func()\n",
    "        model_name += '_{epoch:d}_{val_loss:.2f}.hdf5'\n",
    "    model.summary()\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_name, \n",
    "        monitor='val_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        period=1,\n",
    "        save_weights_only=False)\n",
    "    if fast_train:\n",
    "        epochs = 1\n",
    "        model.fit(train_X, train_Y, epochs=epochs, batch_size=100, validation_split=0.1)\n",
    "    else:\n",
    "        epochs = 20\n",
    "        try:\n",
    "            model.fit(train_X, train_Y, epochs=epochs, batch_size=100, callbacks=[checkpoint], validation_split=0.1)\n",
    "            #model.save('facepoints_model.hdf5')\n",
    "        except KeyboardInterrupt:\n",
    "            #model.save('facepoints_model.hdf5')\n",
    "            print('Training interrupted')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 256)     2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 100, 256)     1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100, 100, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 100, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100, 100, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 25, 25, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 25, 25, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                560028    \n",
      "=================================================================\n",
      "Total params: 988,988\n",
      "Trainable params: 987,900\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n",
      "Train on 5400 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "5400/5400 [==============================] - 59s 11ms/step - loss: 0.1704 - val_loss: 0.0301\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03011, saving model to 1_0.03.hdf5\n",
      "Epoch 2/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.1071 - val_loss: 0.0348\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03011\n",
      "Epoch 3/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.1065 - val_loss: 0.0420\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03011\n",
      "Epoch 4/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0995 - val_loss: 0.0107\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03011 to 0.01068, saving model to 4_0.01.hdf5\n",
      "Epoch 5/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0901 - val_loss: 0.0042\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01068 to 0.00416, saving model to 5_0.00.hdf5\n",
      "Epoch 6/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0802 - val_loss: 0.0214\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00416\n",
      "Epoch 7/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0704 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00416\n",
      "Epoch 8/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0565 - val_loss: 0.0096\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00416\n",
      "Epoch 9/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0655 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00416\n",
      "Epoch 10/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0446 - val_loss: 0.0065\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00416\n",
      "Epoch 11/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0581 - val_loss: 0.0103\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00416\n",
      "Epoch 12/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0530 - val_loss: 0.0124\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00416\n",
      "Epoch 13/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0284 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00416\n",
      "Epoch 14/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0245 - val_loss: 0.0052\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00416\n",
      "Epoch 15/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0215 - val_loss: 0.0031\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00416 to 0.00310, saving model to 15_0.00.hdf5\n",
      "Epoch 16/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0117 - val_loss: 0.0064\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00310\n",
      "Epoch 17/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0049 - val_loss: 0.0106\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00310\n",
      "Epoch 18/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0099 - val_loss: 0.0224\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00310\n",
      "Epoch 19/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0279 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00310\n",
      "Epoch 20/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0164 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00310\n",
      "Epoch 21/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0108 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00310\n",
      "Epoch 22/100\n",
      "5400/5400 [==============================] - 52s 10ms/step - loss: 0.0045 - val_loss: 0.0047\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00310\n",
      "Epoch 23/100\n",
      "1100/5400 [=====>........................] - ETA: 39s - loss: 0.1442"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f2d802aeba8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_detector(train_gt, train_dir+'/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_test():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
