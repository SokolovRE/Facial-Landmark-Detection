{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dir = '00_input/train'\n",
    "im_size = 100\n",
    "coords_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    from numpy import array\n",
    "    res = {}\n",
    "    with open(filename) as fhandle:\n",
    "        next(fhandle)\n",
    "        for line in fhandle:\n",
    "            parts = line.rstrip('\\n').split(',')\n",
    "            coords = array([float(x) for x in parts[1:]], dtype='float64')\n",
    "            res[parts[0]] = coords\n",
    "    return res\n",
    "\n",
    "train_gt = read_csv(train_dir+'/gt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from keras.layers import (Input, concatenate, Conv2D, MaxPooling2D, \n",
    "                          UpSampling2D, Convolution2D, ZeroPadding2D, \n",
    "                          BatchNormalization, Activation, concatenate, \n",
    "                          Flatten, Dense, merge)\n",
    "from keras.optimizers import rmsprop\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input(shape=(im_size, im_size, 1))\n",
    "    conv = Conv2D(filters=256, kernel_size=(3,3), padding='same')(inputs)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=128, kernel_size=(3,3), padding='same')(relu)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    maxpool = MaxPooling2D()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(relu)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    maxpool = MaxPooling2D()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=32, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    flatten = Flatten()(relu)\n",
    "    predictions = Dense(coords_size, activation='tanh')(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def first_model():\n",
    "    inputs = Input(shape=(im_size, im_size, 1))\n",
    "    conv = Conv2D(filters=256, kernel_size=(3,3), padding='same')(inputs)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=128, kernel_size=(3,3), padding='same')(relu)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    maxpool = MaxPooling2D()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(relu)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(relu)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    maxpool = MaxPooling2D()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    conv = Conv2D(filters=64, kernel_size=(3,3), padding='same')(relu)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    maxpool = MaxPooling2D()(relu)\n",
    "    \n",
    "    conv = Conv2D(filters=32, kernel_size=(3,3), padding='same')(maxpool)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    flatten = Flatten()(relu)\n",
    "    predictions = Dense(coords_size, activation='tanh')(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_detector(\n",
    "        train_gt, \n",
    "        train_img_dir, \n",
    "        fast_train=False, \n",
    "        model_func=None, \n",
    "        model_name='{epoch:d}_{val_loss:.4f}.hdf5'):\n",
    "    \n",
    "    def parse(train_gt, train_img_dir, info=False):\n",
    "        from skimage.data import imread\n",
    "        from scipy.ndimage import zoom\n",
    "        train_X = np.zeros((len(train_gt), im_size, im_size, 1))\n",
    "        train_Y = np.zeros((len(train_gt), coords_size))\n",
    "        for i, img_name in enumerate(train_gt):\n",
    "            img = imread(train_img_dir+'/'+img_name, as_grey=True)\n",
    "            train_Y[i] = train_gt[img_name]\n",
    "            for j in range(1, coords_size, 2):\n",
    "                train_Y[i][j] *= im_size/img.shape[0]\n",
    "            for j in range(0, coords_size, 2):\n",
    "                train_Y[i][j] *= im_size/img.shape[1]\n",
    "            train_Y[i] = (train_Y[i] / 100) \n",
    "            img = zoom(img, [im_size/img.shape[0], im_size/img.shape[1]])\n",
    "            img = (img / 255) \n",
    "            train_X[i,:,:,0] = img\n",
    "            del(img)\n",
    "            if info and (i+1)%100 == 0:\n",
    "                print('Image: ', i+1, end='\\r')\n",
    "        return train_X, train_Y\n",
    "    \n",
    "    train_X, train_Y = parse(train_gt, train_img_dir, True)\n",
    "    if model_func == None:\n",
    "        model = get_model()\n",
    "    else:\n",
    "        model = model_func()\n",
    "        model_name += '_{epoch:d}_{val_loss:.4f}.hdf5'\n",
    "    model.summary()\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_name, \n",
    "        monitor='val_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        period=1,\n",
    "        save_weights_only=False)\n",
    "    if fast_train:\n",
    "        epochs = 1\n",
    "        model.fit(train_X, train_Y, epochs=epochs, batch_size=100, validation_split=(1/6))\n",
    "    else:\n",
    "        epochs = 20\n",
    "        try:\n",
    "            model.fit(train_X, train_Y, epochs=epochs, batch_size=100, callbacks=[checkpoint], validation_split=(1/6))\n",
    "            #model.save('facepoints_model.hdf5')\n",
    "        except KeyboardInterrupt:\n",
    "            #model.save('facepoints_model.hdf5')\n",
    "            print('\\nTraining interrupted')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 256)     2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 100, 256)     1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100, 100, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 100, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100, 100, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 25, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                129052    \n",
      "=================================================================\n",
      "Total params: 669,564\n",
      "Trainable params: 668,092\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n",
      "Train on 5400 samples, validate on 600 samples\n",
      "Epoch 1/20\n",
      "5400/5400 [==============================] - 61s 11ms/step - loss: 0.1099 - val_loss: 0.0104\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01035, saving model to first_1_0.0104.hdf5\n",
      "Epoch 2/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0660 - val_loss: 0.0100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01035 to 0.01001, saving model to first_2_0.0100.hdf5\n",
      "Epoch 3/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0478 - val_loss: 0.0116\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01001\n",
      "Epoch 4/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0490 - val_loss: 0.0096\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01001 to 0.00955, saving model to first_4_0.0096.hdf5\n",
      "Epoch 5/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0433 - val_loss: 0.0314\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00955\n",
      "Epoch 6/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0319 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00955\n",
      "Epoch 7/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0307 - val_loss: 0.0093\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00955 to 0.00926, saving model to first_7_0.0093.hdf5\n",
      "Epoch 8/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0227 - val_loss: 0.0087\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00926 to 0.00875, saving model to first_8_0.0087.hdf5\n",
      "Epoch 9/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0085 - val_loss: 0.0056\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00875 to 0.00559, saving model to first_9_0.0056.hdf5\n",
      "Epoch 10/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0120 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00559 to 0.00391, saving model to first_10_0.0039.hdf5\n",
      "Epoch 11/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00391\n",
      "Epoch 12/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0645 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00391\n",
      "Epoch 13/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0376 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00391\n",
      "Epoch 14/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0069 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00391 to 0.00320, saving model to first_14_0.0032.hdf5\n",
      "Epoch 15/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0090 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00320\n",
      "Epoch 16/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0066 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00320\n",
      "Epoch 17/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0207 - val_loss: 0.0076\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00320\n",
      "Epoch 18/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0041 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00320\n",
      "Epoch 19/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0041 - val_loss: 0.0047\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00320\n",
      "Epoch 20/20\n",
      "5400/5400 [==============================] - 54s 10ms/step - loss: 0.0061 - val_loss: 0.0046\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fc4202f9898>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_detector(train_gt, train_dir+'/images', False, first_model, 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect(model, test_img_dir):\n",
    "    from os import listdir\n",
    "    from skimage.data import imread\n",
    "    from scipy.ndimage import zoom\n",
    "    img_list = listdir(test_img_dir)\n",
    "    data = np.zeros((len(img_list), im_size, im_size, 1))\n",
    "    sizes = []\n",
    "    for i, img_name in enumerate(img_list):\n",
    "        img = imread(test_img_dir+'/'+img_name, as_grey=True)\n",
    "        sizes.append([img_name, img.shape])\n",
    "        img = zoom(img, [im_size/img.shape[0], im_size/img.shape[1]])\n",
    "        img = (img / 255)\n",
    "        data[i,:,:,0] = img\n",
    "        del(img)\n",
    "        if(i+1)%100 == 0:\n",
    "            print('Image: ', i+1, end='\\r')\n",
    "    points = model.predict(data, 10, 1)\n",
    "    ans = {}\n",
    "    for i in range(len(points)):\n",
    "        for j in range(1, coords_size, 2):\n",
    "            points[i][j] *= sizes[i][1][0]/im_size\n",
    "            points[i][j] = int(points[i][j])\n",
    "        for j in range(0, coords_size, 2):\n",
    "            points[i][j] *= sizes[i][1][1]/im_size\n",
    "            points[i][j] = int(points[i][j])\n",
    "        ans[sizes[i][0]] = points[i]\n",
    "    return ans\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model()\n",
    "detect(model, '00_input/test/images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
