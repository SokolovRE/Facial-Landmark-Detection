{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from detection import train_detector, detect, read_csv\n",
    "\n",
    "train_dir = '00_input/train'\n",
    "im_size = 100\n",
    "coords_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from keras.layers import (Input, concatenate, Conv2D, MaxPooling2D, \n",
    "                          UpSampling2D, Convolution2D, ZeroPadding2D, \n",
    "                          BatchNormalization, Activation, concatenate, \n",
    "                          Flatten, Dense, merge)\n",
    "from keras.optimizers import rmsprop\n",
    "\n",
    "def first_model():\n",
    "    inputs = Input(shape=(im_size, im_size, 1))\n",
    "    conv = Conv2D(filters=256, kernel_size=(3,3), padding='same')(inputs)\n",
    "    batchnorm = BatchNormalization()(conv)\n",
    "    relu = Activation('relu')(batchnorm)\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    \n",
    "    flatten = Flatten()(relu)\n",
    "    predictions = Dense(coords_size, activation=None)(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g5sokolovroman/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 256)     2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 100, 256)     1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100, 100, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 100, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100, 100, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 25, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 25, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                129052    \n",
      "=================================================================\n",
      "Total params: 669,564\n",
      "Trainable params: 668,092\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n",
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 59s 12ms/step - loss: 296.0593 - val_loss: 248.3737\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 248.37367, saving model to second_1_248.3737.hdf5\n",
      "Epoch 2/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 30.0108 - val_loss: 78.7232\n",
      "\n",
      "Epoch 00002: val_loss improved from 248.37367 to 78.72325, saving model to second_2_78.7232.hdf5\n",
      "Epoch 3/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 25.3926 - val_loss: 167.9076\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 78.72325\n",
      "Epoch 4/20\n",
      "5000/5000 [==============================] - 52s 10ms/step - loss: 21.3295 - val_loss: 49.6713\n",
      "\n",
      "Epoch 00004: val_loss improved from 78.72325 to 49.67126, saving model to second_4_49.6713.hdf5\n",
      "Epoch 5/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 17.7151 - val_loss: 50.9003\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 49.67126\n",
      "Epoch 6/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 16.4790 - val_loss: 44.5584\n",
      "\n",
      "Epoch 00006: val_loss improved from 49.67126 to 44.55835, saving model to second_6_44.5584.hdf5\n",
      "Epoch 7/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 14.8355 - val_loss: 56.2611\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 44.55835\n",
      "Epoch 8/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 13.7295 - val_loss: 29.9421\n",
      "\n",
      "Epoch 00008: val_loss improved from 44.55835 to 29.94210, saving model to second_8_29.9421.hdf5\n",
      "Epoch 9/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 13.2057 - val_loss: 68.2942\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 29.94210\n",
      "Epoch 10/20\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 12.3767 - val_loss: 103.8147\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 29.94210\n",
      "Epoch 11/20\n",
      " 400/5000 [=>............................] - ETA: 44s - loss: 12.2625\n",
      "Training interrupted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f55505cdc50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gt = read_csv(train_dir+'/gt.csv')\n",
    "train_detector(train_gt, train_dir+'/images', False, first_model, 'second')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
